{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from members import Members\n",
    "from weather import Weather\n",
    "from expeds import Expeds\n",
    "\n",
    "\n",
    "# Get Data\n",
    "member = Members().get_data()\n",
    "member = Members().clean_data(member)\n",
    "\n",
    "weather = Weather().get_data()\n",
    "weather = Weather().clean_data(weather)\n",
    "\n",
    "exped = Expeds().get_data()\n",
    "exped = Expeds().clean_data(exped)\n",
    "\n",
    "# Drop columns\n",
    "mem_to_drop = ['memb_id','year','unique_id','peak_id','residence','occupation',\n",
    "'summit_claimed','summit_disputed','highpt','high_point','death','death_type',\n",
    "'death_height','death_class','summit_bid','summit_term','summit_date1',\n",
    "'citizenship','o2_climb','o2_descent','o2_sleep','o2_medical', 'o2_none', \n",
    "'yob', 'route1', 'ascent1', 'leader', 'deputy', 'bconly', 'nottobc', 'support', \n",
    "'hired', 'sherpa', 'tibetan']\n",
    "\n",
    "\n",
    "exp_to_drop = ['year','season','route1','route2','nation','leaders',\n",
    "'sponsor','success1','success2', 'ascent1','claimed','disputed',\n",
    "'countries','summit_time','term_date','term_note','high_point',\n",
    "'traverse','ski','parapente','o2_climb','o2_descent','o2_sleep',\n",
    "'o2_medical','o2_taken','o2_unkwn','o2_used','o2_none','other_smts',\n",
    "'campsites','accidents','achievment','agency','peak_name','primmem',\n",
    "'summiter_deaths','summit_members','summit_hired','hired_deaths']\n",
    "\n",
    "member.drop(columns= mem_to_drop, inplace=True)\n",
    "exped.drop(columns= exp_to_drop, inplace=True)\n",
    "\n",
    "exped['summit_date'] = pd.to_datetime(exped.summit_date, errors = 'coerce')\n",
    "exped['bc_date'] = pd.to_datetime(exped.bc_date , errors = 'coerce')\n",
    "exped = exped.set_index('summit_date')\n",
    "weather = weather.set_index('date_time')\n",
    "\n",
    "# Feature Engineering (1/2)\n",
    "exped['sherpa_ratio'] = exped['tot_hired'] / exped['tot_members']\n",
    "exped['sherpa_ratio'] = np.where(exped['sherpa_ratio'] == np.inf, 0, exped['sherpa_ratio'])\n",
    "\n",
    "weather['pressure_past'] = weather['pressure'].rolling(window=3).mean()\n",
    "weather['pressure_futur'] = weather['pressure'].shift(-2).rolling(window=3).mean()\n",
    "weather['stability'] = weather['pressure_futur'] - weather['pressure_past']\n",
    "\n",
    "# Merge DataFrames\n",
    "df = exped.merge(weather, how='left', left_index=True, right_index=True)\n",
    "df = df.reset_index()\n",
    "df = df.rename(columns={'index' : 'summit_date'})\n",
    "df = df.merge(member, on='exp_id', how = 'right')\n",
    "df.drop(columns=['moonrise', 'moonset', 'sunrise', 'sunset'], inplace = True)\n",
    "df = df.dropna(subset=['summit_date', 'bc_date'])\n",
    "\n",
    "# Feature Engineering (2/2)\n",
    "df['cumul_snow'] = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    date1 = row['bc_date'].date()\n",
    "    date2 = row['summit_date'].date()\n",
    "    acc_snow = weather.loc[date1:date2, 'totalSnow_cm'].sum()\n",
    "    df.loc[index, 'cumul_snow'] = acc_snow\n",
    "\n",
    "df.drop(columns=['summit_date', 'exp_id', 'bc_date', 'term_reason'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing\n",
    "col_num = []\n",
    "col_bool =[]\n",
    "col_object =[]\n",
    "\n",
    "for col in df:\n",
    "    if df[col].dtype == \"float64\":\n",
    "        col_num.append(col)\n",
    "        \n",
    "    if df[col].dtype == \"int64\":\n",
    "        col_num.append(col)\n",
    "        \n",
    "    if df[col].dtype == 'bool':\n",
    "        col_bool.append(col)\n",
    "        \n",
    "    if df[col].dtype == 'object':\n",
    "        col_object.append(col)\n",
    "        \n",
    "col_bool.remove('summit_success')\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer()),\n",
    "    ('scaler', MinMaxScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop= 'first', handle_unknown='error'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, col_num),\n",
    "        ('cat', categorical_transformer, col_object)])\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "X = df.drop(columns=['summit_success'])\n",
    "y = df.summit_success\n",
    "\n",
    "X_trans = clf.fit_transform(X)\n",
    "X_tr = X_trans.toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tr, y, test_size= 0.3, random_state= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.81      0.83      2290\n",
      "        True       0.78      0.83      0.80      1852\n",
      "\n",
      "    accuracy                           0.82      4142\n",
      "   macro avg       0.82      0.82      0.82      4142\n",
      "weighted avg       0.82      0.82      0.82      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model Traning\n",
    "from sklearn.metrics import classification_report\n",
    "boost = XGBClassifier()\n",
    "boost.fit(X_train, y_train)\n",
    "ypred = boost.predict(X_test)\n",
    "print(classification_report(y_test, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.82      0.83      2290\n",
      "        True       0.78      0.81      0.79      1852\n",
      "\n",
      "    accuracy                           0.81      4142\n",
      "   macro avg       0.81      0.81      0.81      4142\n",
      "weighted avg       0.81      0.81      0.81      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Best model of Jeremy\n",
    "best_of_best = XGBClassifier(n_estimators = 300,\n",
    "                            max_depth = 18,\n",
    "                            learning_rate = 0.3,\n",
    "                            gamma = 0.8,\n",
    "                            min_child_weight = 4,\n",
    "                            colsample_bytree = 0.3\n",
    "                            )\n",
    "\n",
    "best_of_best.fit(X_train, y_train)\n",
    "ypred = best_of_best.predict(X_test)\n",
    "print(classification_report(y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      0.76      0.75      2290\n",
      "        True       0.69      0.66      0.67      1852\n",
      "\n",
      "    accuracy                           0.72      4142\n",
      "   macro avg       0.71      0.71      0.71      4142\n",
      "weighted avg       0.72      0.72      0.72      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "reg = LogisticRegression(max_iter=500)\n",
    "reg.fit(X_train, y_train)\n",
    "ypred = reg.predict(X_test)\n",
    "print(classification_report(y_test, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.84      0.81      0.82      2290\n",
      "        True       0.77      0.80      0.79      1852\n",
      "\n",
      "    accuracy                           0.81      4142\n",
      "   macro avg       0.80      0.81      0.80      4142\n",
      "weighted avg       0.81      0.81      0.81      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "ypred = clf.predict(X_test)\n",
    "print(classification_report(y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RandomSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None, gamma=None,\n",
       "                                           gpu_id=None, importance_type='gain',\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100,...\n",
       "                   param_distributions={'booster': ['gbtree', 'gblinear'],\n",
       "                                        'colsample_bytree': [0.6, 0.8, 1.0],\n",
       "                                        'gamma': [0.5, 1, 1.5, 2, 5],\n",
       "                                        'learning_rate': [0.0001, 0.001, 0.1,\n",
       "                                                          1],\n",
       "                                        'max_depth': [5, 10, 25, 50],\n",
       "                                        'min_child_weight': [0.1, 1, 5, 10, 50],\n",
       "                                        'n_estimators': [50, 100, 250, 500],\n",
       "                                        'reg_alpha': [0.0001, 0.001, 0.1, 1],\n",
       "                                        'reg_lambda': [0.0001, 0.001, 0.1, 1],\n",
       "                                        'subsample': [0.6, 0.8, 1.0]},\n",
       "                   random_state=1001, scoring='accuracy')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {'booster' : ['gbtree', 'gblinear'],\n",
    "        'min_child_weight': [0.1, 1, 5, 10, 50],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [5, 10, 25, 50],\n",
    "        'learning_rate': [0.0001, 0.001, 0.1, 1],\n",
    "        'n_estimators': [50, 100, 250, 500],\n",
    "        'reg_alpha': [0.0001, 0.001, 0.1, 1],\n",
    "        'reg_lambda': [0.0001, 0.001, 0.1, 1]\n",
    "        }\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(model, \n",
    "                                   cv=5,\n",
    "                                   param_distributions=params,  \n",
    "                                   scoring='accuracy', \n",
    "                                   n_jobs=-1,\n",
    "                                   verbose=1, \n",
    "                                   random_state=1001)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.6, gamma=2, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.001, max_delta_step=0, max_depth=25,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0.1, reg_lambda=0.0001, scale_pos_weight=1,\n",
       "              subsample=0.8, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.81      0.83      2290\n",
      "        True       0.78      0.84      0.81      1852\n",
      "\n",
      "    accuracy                           0.82      4142\n",
      "   macro avg       0.82      0.82      0.82      4142\n",
      "weighted avg       0.82      0.82      0.82      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   19.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['gini', 'entropy'],\n",
       "                                        'max_depth': [5, 10, 25, 50],\n",
       "                                        'max_features': ['sqrt', 'log2'],\n",
       "                                        'min_samples_leaf': [1, 3, 5],\n",
       "                                        'min_samples_split': [2, 3, 5],\n",
       "                                        'n_estimators': [100, 200, 500]},\n",
       "                   random_state=1001, scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "params = {'n_estimators' : [100,200,500],\n",
    "        'criterion' : ['gini', 'entropy'],\n",
    "        'min_samples_split': [2, 3, 5],\n",
    "        'max_depth': [5, 10, 25, 50],\n",
    "        'min_samples_leaf': [1, 3, 5],\n",
    "        'max_features': [\"sqrt\", \"log2\"]\n",
    "        }\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(model, \n",
    "                                   cv=5,\n",
    "                                   param_distributions=params,  \n",
    "                                   scoring='accuracy', \n",
    "                                   n_jobs=-1,\n",
    "                                   verbose=1, \n",
    "                                   random_state=1001)\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=25, max_features='log2',\n",
       "                       min_samples_split=5, n_estimators=200)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.86      0.81      0.83      2290\n",
      "        True       0.78      0.83      0.80      1852\n",
      "\n",
      "    accuracy                           0.82      4142\n",
      "   macro avg       0.82      0.82      0.82      4142\n",
      "weighted avg       0.82      0.82      0.82      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Test xgb_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from xgb_model import HimalXGB\n",
    "\n",
    "model = HimalXGB().train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = HimalXGB().predict_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.88      0.90      7590\n",
      "        True       0.86      0.91      0.89      6214\n",
      "\n",
      "    accuracy                           0.90     13804\n",
      "   macro avg       0.89      0.90      0.89     13804\n",
      "weighted avg       0.90      0.90      0.90     13804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
